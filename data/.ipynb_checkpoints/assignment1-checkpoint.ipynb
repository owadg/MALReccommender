{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301bf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cab31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f967ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.8 64-bit (microsoft store)' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Zhiend/AppData/Local/Microsoft/WindowsApps/python3.10.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e25a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88efc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e50df1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "09ac1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allRatings = []\n",
    "for l in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    allRatings.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e4717806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "ca3c2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsTrain = allRatings[:190000]\n",
    "ratingsValid = allRatings[190000:]\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "ratingDict = defaultdict(int)\n",
    "\n",
    "itemSet = set()\n",
    "userSet = set()\n",
    "for u,b,r in ratingsTrain:\n",
    "    ratingsPerUser[u].append((b,r))\n",
    "    ratingsPerItem[b].append((u,r))\n",
    "    ratingDict[(u,b)] = r\n",
    "    itemSet.add(b)\n",
    "    userSet.add(u)\n",
    "itemList = list(itemSet)\n",
    "userList = list(userSet)\n",
    "\n",
    "itemAverages = defaultdict(int)\n",
    "for i in ratingsPerItem:\n",
    "    if len(ratingsPerItem[i]) > 0:\n",
    "        itemAverages[i] = sum(list(zip(*ratingsPerItem[i]))[1]) / len(ratingsPerItem[i])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "20d08409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('u67805239', 'b61372131', 4)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsTrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "93959f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Read prediction                                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "59260457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we attempt a model based on Bayesian Personal Ranking implemented by implicit. \n",
    "#If it is similar or reccommended we will predict that it is a read pair\n",
    "#if the user is not in the dataset, we will guess \n",
    "#this is lifted straight from pml_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "80f40789",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7c9eea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just going to jump ship on the BPR model and opt for a simpler heuristic model\n",
    "#such as the one used in hw3 - popularity based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35dbc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1\n",
    "#helper func for generating newValidation\n",
    "def getUnreadBook(user):\n",
    "    readBefore = True\n",
    "    while (readBefore):\n",
    "        readBefore = False\n",
    "        book = itemList[random.randint(0, len(itemList)-1)]\n",
    "        a = ratingsPerUser[user]\n",
    "        for pair in a:\n",
    "            if(pair[0] == book):\n",
    "                readBefore = True\n",
    "    return book\n",
    "\n",
    "#add negatives to  validation set\n",
    "def generateNewValidation(oldValidation):\n",
    "    random.seed()\n",
    "    newValidation = oldValidation.copy()\n",
    "    length = len(oldValidation)\n",
    "    for i in range(length):\n",
    "        l = oldValidation[i]\n",
    "        #get book user hasn't read. \n",
    "        book = getUnreadBook(u)\n",
    "        j = (l[0],book,0)\n",
    "        \n",
    "        newValidation.append(j)\n",
    "    return newValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7b45fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from baseline code\n",
    "def generatePopSet(thresholdPercentage):\n",
    "    bookCount = defaultdict(int)\n",
    "    totalRead = 0\n",
    "\n",
    "    for user,book,_ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "        bookCount[book] += 1\n",
    "        totalRead += 1\n",
    "\n",
    "    mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "    mostPopular.sort()\n",
    "    mostPopular.reverse()\n",
    "\n",
    "    popset = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        popset.add(i)\n",
    "        if count > totalRead*thresholdPercentage: return popset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "11cbae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(popset, validationSet):\n",
    "    ##predictions\n",
    "    #mke predictions\n",
    "    predValid = []\n",
    "    for l in validationSet:\n",
    "        u = l[0]\n",
    "        b = l[1]\n",
    "        if b in popset:\n",
    "            predValid.append([u, b, 1])\n",
    "        else:\n",
    "            predValid.append([u, b, 0])\n",
    "    \n",
    "    ##accuracy\n",
    "    correct = 0\n",
    "    for i in range(len(predValid)):\n",
    "        #the first 10000 have actually been read\n",
    "        if i < 10000:\n",
    "            if predValid[i][2] == 1:\n",
    "                correct += 1\n",
    "        #the rest havent been read\n",
    "        else:\n",
    "            if predValid[i][2] == 0:\n",
    "                correct += 1\n",
    "    acc1 = correct / len(predValid)\n",
    "    return acc1\n",
    "def accuracy(predValid):\n",
    "    correct = 0\n",
    "    for i in range(len(predValid)):\n",
    "        #the first half have actually been read\n",
    "        if i < len(predValid)//2:\n",
    "            if predValid[i][2] == 1:\n",
    "                correct += 1\n",
    "        #the rest havent been read\n",
    "        else:\n",
    "            if predValid[i][2] == 0:\n",
    "                correct += 1\n",
    "    acc1 = correct / len(predValid)\n",
    "    return acc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5df5cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the similarity functions have a dict that serves to function as cache for fast computation\n",
    "\n",
    "#cache\n",
    "jaccarddict = dict()\n",
    "def Jaccard(i1, i2):\n",
    "    if (i1,i2) in jaccarddict:\n",
    "        return jaccarddict[(i1,i2)]\n",
    "    elif (i2,i1) in jaccarddict:\n",
    "        return jaccarddict[(i2,i1)]\n",
    "    s1 = set(list(zip(*ratingsPerItem[i1]))[0])\n",
    "    s2 = set(list(zip(*ratingsPerItem[i2]))[0])\n",
    "    num = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        jaccarddict[(i2,i1)] = 0\n",
    "        return 0\n",
    "    else:\n",
    "        jaccarddict[(i2,i1)] = len(s1.intersection(s2))/ len(s1.union(s2))\n",
    "        return len(s1.intersection(s2))/ len(s1.union(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "ab045381",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdict = dict()\n",
    "def Cosine (i1 , i2):\n",
    "    if (i1,i2) in cosdict:\n",
    "        return cosdict[(i1,i2)]\n",
    "    elif (i2,i1) in cosdict:\n",
    "        return cosdict[(i2,i1)]\n",
    "# Between two items\n",
    "    i1Set = set(list(zip(*ratingsPerItem[i1]))[0])\n",
    "    i2Set = set(list(zip(*ratingsPerItem[i2]))[0])\n",
    "    inter = i1Set.intersection(i1Set)\n",
    "    numer = sum ([ratingDict[(u,i1)]* ratingDict [(u,i2)] for u in inter])\n",
    "    norm1 = sum ([ratingDict[(u,i1)]**2 for u in i1Set])\n",
    "    norm2 = sum ([ratingDict[(u,i2)]**2 for u in i2Set])\n",
    "    denom = math.sqrt( norm1 ) * math.sqrt( norm2 )\n",
    "    if denom == 0: \n",
    "        cosdict[(i2,i1)] = 0\n",
    "        return 0 # If one of the two items has no ratings\n",
    "    cosdict[(i2,i1)] = numer/denom\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "1d2a7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsondict = dict()\n",
    "def Pearson (i1 , i2):\n",
    "    if (i1,i2) in pearsondict:\n",
    "        return pearsondict[(i1,i2)]\n",
    "    elif (i2,i1) in pearsondict:\n",
    "        return pearsondict[(i2,i1)]\n",
    "    # Between two items\n",
    "    iBar1 , iBar2 = itemAverages[i1], itemAverages[i2]\n",
    "    i1Set = set(list(zip(*ratingsPerItem[i1]))[0])\n",
    "    i2Set = set(list(zip(*ratingsPerItem[i2]))[0])\n",
    "    inter = i1Set.intersection(i1Set)    \n",
    "    numer = 0\n",
    "    denom1 = 0\n",
    "    denom2 = 0\n",
    "    for u in inter :\n",
    "        numer += ( ratingDict [(u,i1)] - iBar1 )*( ratingDict [(u,i2)] - iBar2 )\n",
    "    for u in inter : # Alternately could sum over usersPerItem [i1 ]/[ i2]\n",
    "        denom1 += ( ratingDict [(u,i1)] - iBar1 )**2\n",
    "        denom2 += ( ratingDict [(u,i2)] - iBar2 )**2\n",
    "    denom = math.sqrt( denom1 ) * math.sqrt( denom2 )\n",
    "    if denom == 0: \n",
    "        pearsondict[(i2,i1)] = 0\n",
    "        return 0\n",
    "    pearsondict[(i2,i1)] = numer/denom\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "9b5c711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity check then jac check\n",
    "def makePrediction(jacThreshold, popset, validationSet):\n",
    "    count = 0\n",
    "    predValid = []\n",
    "    for l in validationSet:\n",
    "        u = l[0]\n",
    "        b = l[1]\n",
    "        if b in popset:\n",
    "            #it's popular ish, so they probably read it\n",
    "            predValid.append([u, b, 1])\n",
    "        else:\n",
    "            #it's not popular. let's dive deeper into our predictive task\n",
    "            #and use a similarity function.\n",
    "            #we need to make sure we have seen this user before, otherwise we can't do jaccard\n",
    "            #we also need to make sure this book has been read by someone.\n",
    "            #if we havent seent this user or book before, lets assume they wont read it.\n",
    "            #Note the above assumption will be correct half the time we don't see the user. or book\n",
    "            if len(ratingsPerUser[u]) == 0 or len(ratingsPerItem[b]) ==  0:\n",
    "                #print('count ' + str(count)+ ' num user ratings ' + str(len(ratingsPerUser[u]))+ ' num item ratings ' + str(len(ratingsPerItem[b])))\n",
    "                #print(u + ', ' + b)\n",
    "                predValid.append([u, b, 0])\n",
    "                count += 1\n",
    "                continue\n",
    "            #Now we do the maxJac test but modified so as to do sum of the jaccards. \n",
    "            #if the user reads a lot of books, the sum will probably be higher\n",
    "            #if they read a lot of books, then it is morelikey they have read this book.\n",
    "            \n",
    "            #get jaccard similarities\n",
    "            jacs = []\n",
    "            booksUserHasRead = list(zip(*ratingsPerUser[u]))[0]\n",
    "            for book in booksUserHasRead:\n",
    "                jacs.append(Jaccard(book, b))\n",
    "            sumJac = sum(jacs)\n",
    "            #print('index: ' + str(count) + ' sumJac: ' + str(sumJac))\n",
    "            if sumJac > jacThreshold:\n",
    "                predValid.append([u, b, 1])\n",
    "            else:\n",
    "                predValid.append([u, b, 0])\n",
    "        count += 1\n",
    "    return predValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "f4bcb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity check then cos check\n",
    "def makePrediction1(threshold, popset, validationSet):\n",
    "    count = 0\n",
    "    predValid = []\n",
    "    for l in validationSet:\n",
    "        u = l[0]\n",
    "        b = l[1]\n",
    "        if b in popset:\n",
    "            #it's popular ish, so they probably read it\n",
    "            predValid.append([u, b, 1])\n",
    "        else:\n",
    "            #it's not popular. let's dive deeper into our predictive task\n",
    "            #and use a similarity function.\n",
    "            #we need to make sure we have seen this user before, otherwise we can't do jaccard\n",
    "            #we also need to make sure this book has been read by someone.\n",
    "            #if we havent seent this user or book before, lets assume they wont read it.\n",
    "            #Note the above assumption will be correct half the time we don't see the user. or book\n",
    "            if len(ratingsPerUser[u]) == 0 or len(ratingsPerItem[b]) ==  0:\n",
    "                #print('count ' + str(count)+ ' num user ratings ' + str(len(ratingsPerUser[u]))+ ' num item ratings ' + str(len(ratingsPerItem[b])))\n",
    "                #print(u + ', ' + b)\n",
    "                predValid.append([u, b, 0])\n",
    "                count += 1\n",
    "                continue\n",
    "            #Now we do the maxJac test but modified so as to do sum of the jaccards. \n",
    "            #if the user reads a lot of books, the sum will probably be higher\n",
    "            #if they read a lot of books, then it is morelikey they have read this book.\n",
    "            \n",
    "            #get jaccard similarities\n",
    "            coss = []\n",
    "            booksUserHasRead = list(zip(*ratingsPerUser[u]))[0]\n",
    "            for book in booksUserHasRead:\n",
    "                coss.append(Cosine(book, b))\n",
    "            sumCos = sum(coss)\n",
    "            #print('index: ' + str(count) + ' sumJac: ' + str(sumJac))\n",
    "            if sumCos > threshold:\n",
    "                predValid.append([u, b, 1])\n",
    "            else:\n",
    "                predValid.append([u, b, 0])\n",
    "        count += 1\n",
    "    return predValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "199f5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity check then pearson check\n",
    "def makePrediction2(threshold, popset, validationSet):\n",
    "    count = 0\n",
    "    predValid = []\n",
    "    for l in validationSet:\n",
    "        u = l[0]\n",
    "        b = l[1]\n",
    "        if b in popset:\n",
    "            #it's popular ish, so they probably read it\n",
    "            predValid.append([u, b, 1])\n",
    "        else:\n",
    "            #it's not popular. let's dive deeper into our predictive task\n",
    "            #and use a similarity function.\n",
    "            #we need to make sure we have seen this user before, otherwise we can't do jaccard\n",
    "            #we also need to make sure this book has been read by someone.\n",
    "            #if we havent seent this user or book before, lets assume they wont read it.\n",
    "            #Note the above assumption will be correct half the time we don't see the user. or book\n",
    "            if len(ratingsPerUser[u]) == 0 or len(ratingsPerItem[b]) ==  0:\n",
    "                #print('count ' + str(count)+ ' num user ratings ' + str(len(ratingsPerUser[u]))+ ' num item ratings ' + str(len(ratingsPerItem[b])))\n",
    "                #print(u + ', ' + b)\n",
    "                predValid.append([u, b, 0])\n",
    "                count += 1\n",
    "                continue\n",
    "            #Now we do the maxJac test but modified so as to do sum of the jaccards. \n",
    "            #if the user reads a lot of books, the sum will probably be higher\n",
    "            #if they read a lot of books, then it is morelikey they have read this book.\n",
    "            \n",
    "            #get jaccard similarities\n",
    "            coss = []\n",
    "            booksUserHasRead = list(zip(*ratingsPerUser[u]))[0]\n",
    "            for book in booksUserHasRead:\n",
    "                coss.append(Pearson(book, b))\n",
    "            sumCos = sum(coss)\n",
    "            #print('index: ' + str(count) + ' sumJac: ' + str(sumJac))\n",
    "            if sumCos > threshold:\n",
    "                predValid.append([u, b, 1])\n",
    "            else:\n",
    "                predValid.append([u, b, 0])\n",
    "        count += 1\n",
    "    return predValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "61c872d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity check baseline\n",
    "def makePrediction0(popThreshold, validationSet):\n",
    "    count = 0\n",
    "    predValid = []\n",
    "    popset = generatePopSet(popThreshold)\n",
    "    for l in validationSet:\n",
    "        u = l[0]\n",
    "        b = l[1]\n",
    "        if b in popset:\n",
    "            #it's popular ish, so they probably read it\n",
    "            predValid.append([u, b, 1])\n",
    "        else:\n",
    "            predValid.append([u, b, 0])\n",
    "        count += 1\n",
    "    return predValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "8d6decbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate validation set\n",
    "nv = generateNewValidation(ratingsValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "240dfaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75435"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline\n",
    "accuracy(makePrediction0(0.74, nv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "41193cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "71\n",
      "72\n",
      "72% done\n",
      "73\n",
      "74\n",
      "75\n",
      "CPU times: total: 10min 39s\n",
      "Wall time: 10min 56s\n"
     ]
    }
   ],
   "source": [
    "#SquareSearch through parameters\n",
    "acclist = []\n",
    "#jac threshhold\n",
    "for i in range(70, 76):\n",
    "    print(i)\n",
    "    #pop threshold\n",
    "    popSet = generatePopSet(i/100)\n",
    "    for j in range (0, 150):\n",
    "        acc1 = accuracy(makePrediction(j/1000, popSet, nv))\n",
    "        acc2 = accuracy(makePrediction1(j/1000, popSet, nv))\n",
    "        acc3 = accuracy(makePrediction2(j/1000, popSet, nv))\n",
    "        acclist.append((acc1, j/1000, i/100, 1))\n",
    "        acclist.append((acc2, j/1000, i/100, 2))\n",
    "        acclist.append((acc3, j/1000, i/100, 3))\n",
    "    if i % 4 == 0:\n",
    "        print(str(i) + '% done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "e9489a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "acclist.sort()\n",
    "acclist.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "51a8dde1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7553, 0.057, 0.72, 3),\n",
       " (0.7553, 0.056, 0.72, 3),\n",
       " (0.75525, 0.059, 0.72, 3),\n",
       " (0.75525, 0.058, 0.72, 3),\n",
       " (0.75525, 0.055, 0.72, 3),\n",
       " (0.75525, 0.05, 0.72, 1),\n",
       " (0.75525, 0.033, 0.72, 3),\n",
       " (0.7552, 0.06, 0.72, 3),\n",
       " (0.7552, 0.054, 0.72, 3),\n",
       " (0.7552, 0.051, 0.72, 1),\n",
       " (0.75515, 0.065, 0.72, 1),\n",
       " (0.75515, 0.049, 0.72, 3),\n",
       " (0.75515, 0.049, 0.72, 1),\n",
       " (0.75515, 0.046, 0.72, 3),\n",
       " (0.75515, 0.034, 0.72, 3),\n",
       " (0.75515, 0.032, 0.72, 3),\n",
       " (0.7551, 0.073, 0.72, 3),\n",
       " (0.7551, 0.065, 0.74, 1),\n",
       " (0.7551, 0.063, 0.72, 3),\n",
       " (0.7551, 0.055, 0.72, 1),\n",
       " (0.7551, 0.053, 0.72, 3),\n",
       " (0.7551, 0.052, 0.72, 1),\n",
       " (0.7551, 0.05, 0.74, 1),\n",
       " (0.7551, 0.048, 0.72, 3),\n",
       " (0.7551, 0.048, 0.72, 1),\n",
       " (0.7551, 0.047, 0.72, 3),\n",
       " (0.7551, 0.045, 0.72, 3),\n",
       " (0.7551, 0.032, 0.72, 1),\n",
       " (0.7551, 0.029, 0.72, 3),\n",
       " (0.75505, 0.078, 0.72, 3),\n",
       " (0.75505, 0.077, 0.72, 3),\n",
       " (0.75505, 0.075, 0.72, 3),\n",
       " (0.75505, 0.074, 0.72, 3),\n",
       " (0.75505, 0.072, 0.72, 3),\n",
       " (0.75505, 0.067, 0.72, 3),\n",
       " (0.75505, 0.063, 0.72, 1),\n",
       " (0.75505, 0.062, 0.72, 3),\n",
       " (0.75505, 0.061, 0.72, 3),\n",
       " (0.75505, 0.057, 0.72, 1),\n",
       " (0.75505, 0.056, 0.72, 1),\n",
       " (0.75505, 0.054, 0.72, 1),\n",
       " (0.75505, 0.053, 0.72, 1),\n",
       " (0.75505, 0.052, 0.72, 3),\n",
       " (0.75505, 0.051, 0.74, 1),\n",
       " (0.75505, 0.051, 0.72, 3),\n",
       " (0.75505, 0.05, 0.72, 3),\n",
       " (0.75505, 0.035, 0.72, 3),\n",
       " (0.75505, 0.03, 0.72, 3),\n",
       " (0.75505, 0.028, 0.72, 3),\n",
       " (0.755, 0.079, 0.72, 3),\n",
       " (0.755, 0.076, 0.72, 3),\n",
       " (0.755, 0.069, 0.72, 3),\n",
       " (0.755, 0.068, 0.72, 3),\n",
       " (0.755, 0.066, 0.72, 3),\n",
       " (0.755, 0.066, 0.72, 1),\n",
       " (0.755, 0.065, 0.72, 3),\n",
       " (0.755, 0.064, 0.72, 3),\n",
       " (0.755, 0.064, 0.72, 1),\n",
       " (0.755, 0.063, 0.74, 1),\n",
       " (0.755, 0.062, 0.72, 1),\n",
       " (0.755, 0.061, 0.72, 1),\n",
       " (0.755, 0.059, 0.72, 1),\n",
       " (0.755, 0.057, 0.74, 1),\n",
       " (0.755, 0.055, 0.74, 1),\n",
       " (0.755, 0.052, 0.74, 1),\n",
       " (0.755, 0.042, 0.72, 3),\n",
       " (0.755, 0.031, 0.72, 3),\n",
       " (0.75495, 0.08, 0.72, 3),\n",
       " (0.75495, 0.071, 0.72, 3),\n",
       " (0.75495, 0.07, 0.72, 3),\n",
       " (0.75495, 0.066, 0.74, 1),\n",
       " (0.75495, 0.064, 0.74, 1),\n",
       " (0.75495, 0.062, 0.74, 1),\n",
       " (0.75495, 0.061, 0.74, 1),\n",
       " (0.75495, 0.06, 0.72, 1),\n",
       " (0.75495, 0.059, 0.74, 1),\n",
       " (0.75495, 0.058, 0.72, 1),\n",
       " (0.75495, 0.057, 0.74, 3),\n",
       " (0.75495, 0.056, 0.74, 3),\n",
       " (0.75495, 0.056, 0.74, 1),\n",
       " (0.75495, 0.054, 0.74, 1),\n",
       " (0.75495, 0.053, 0.74, 1),\n",
       " (0.75495, 0.05, 0.73, 1),\n",
       " (0.75495, 0.049, 0.74, 1),\n",
       " (0.75495, 0.044, 0.72, 3),\n",
       " (0.75495, 0.043, 0.72, 3),\n",
       " (0.75495, 0.039, 0.72, 3),\n",
       " (0.75495, 0.036, 0.72, 3),\n",
       " (0.75495, 0.031, 0.72, 1),\n",
       " (0.7549, 0.096, 0.72, 3),\n",
       " (0.7549, 0.095, 0.72, 3),\n",
       " (0.7549, 0.094, 0.72, 3),\n",
       " (0.7549, 0.093, 0.72, 3),\n",
       " (0.7549, 0.092, 0.72, 3),\n",
       " (0.7549, 0.091, 0.72, 3),\n",
       " (0.7549, 0.09, 0.72, 3),\n",
       " (0.7549, 0.089, 0.72, 3),\n",
       " (0.7549, 0.06, 0.74, 1),\n",
       " (0.7549, 0.059, 0.74, 3),\n",
       " (0.7549, 0.058, 0.74, 3),\n",
       " (0.7549, 0.058, 0.74, 1),\n",
       " (0.7549, 0.057, 0.73, 3),\n",
       " (0.7549, 0.056, 0.73, 3),\n",
       " (0.7549, 0.055, 0.74, 3),\n",
       " (0.7549, 0.051, 0.73, 1),\n",
       " (0.7549, 0.041, 0.72, 3),\n",
       " (0.75485, 0.081, 0.72, 3),\n",
       " (0.75485, 0.071, 0.74, 1),\n",
       " (0.75485, 0.071, 0.72, 1),\n",
       " (0.75485, 0.067, 0.72, 1),\n",
       " (0.75485, 0.065, 0.73, 1),\n",
       " (0.75485, 0.06, 0.74, 3),\n",
       " (0.75485, 0.059, 0.73, 3),\n",
       " (0.75485, 0.058, 0.73, 3),\n",
       " (0.75485, 0.055, 0.73, 3),\n",
       " (0.75485, 0.054, 0.74, 3),\n",
       " (0.75485, 0.048, 0.74, 1),\n",
       " (0.75485, 0.047, 0.72, 1),\n",
       " (0.75485, 0.026, 0.72, 3),\n",
       " (0.7548, 0.098, 0.72, 3),\n",
       " (0.7548, 0.097, 0.72, 3),\n",
       " (0.7548, 0.088, 0.72, 3),\n",
       " (0.7548, 0.082, 0.72, 3),\n",
       " (0.7548, 0.067, 0.74, 1),\n",
       " (0.7548, 0.063, 0.74, 3),\n",
       " (0.7548, 0.06, 0.73, 3),\n",
       " (0.7548, 0.055, 0.73, 1),\n",
       " (0.7548, 0.054, 0.73, 3),\n",
       " (0.7548, 0.052, 0.73, 1),\n",
       " (0.7548, 0.049, 0.73, 1),\n",
       " (0.7548, 0.046, 0.72, 1),\n",
       " (0.7548, 0.045, 0.72, 1),\n",
       " (0.7548, 0.04, 0.72, 3),\n",
       " (0.7548, 0.033, 0.73, 3),\n",
       " (0.7548, 0.033, 0.72, 1),\n",
       " (0.7548, 0.027, 0.72, 3),\n",
       " (0.75475, 0.087, 0.72, 3),\n",
       " (0.75475, 0.084, 0.72, 3),\n",
       " (0.75475, 0.083, 0.72, 3),\n",
       " (0.75475, 0.078, 0.74, 3),\n",
       " (0.75475, 0.077, 0.74, 3),\n",
       " (0.75475, 0.075, 0.74, 3),\n",
       " (0.75475, 0.073, 0.74, 3),\n",
       " (0.75475, 0.07, 0.74, 1),\n",
       " (0.75475, 0.07, 0.72, 1),\n",
       " (0.75475, 0.068, 0.72, 1),\n",
       " (0.75475, 0.063, 0.73, 3),\n",
       " (0.75475, 0.063, 0.73, 1),\n",
       " (0.75475, 0.062, 0.74, 3),\n",
       " (0.75475, 0.061, 0.74, 3),\n",
       " (0.75475, 0.057, 0.73, 1),\n",
       " (0.75475, 0.056, 0.73, 1),\n",
       " (0.75475, 0.054, 0.73, 1),\n",
       " (0.75475, 0.053, 0.74, 3),\n",
       " (0.75475, 0.053, 0.73, 1),\n",
       " (0.75475, 0.048, 0.73, 1),\n",
       " (0.75475, 0.046, 0.74, 3),\n",
       " (0.75475, 0.044, 0.72, 1),\n",
       " (0.75475, 0.04, 0.72, 1),\n",
       " (0.75475, 0.038, 0.72, 3),\n",
       " (0.75475, 0.037, 0.72, 3),\n",
       " (0.75475, 0.036, 0.72, 1),\n",
       " (0.7547, 0.119, 0.72, 3),\n",
       " (0.7547, 0.118, 0.72, 3),\n",
       " (0.7547, 0.117, 0.72, 3),\n",
       " (0.7547, 0.115, 0.72, 3),\n",
       " (0.7547, 0.114, 0.72, 3),\n",
       " (0.7547, 0.113, 0.72, 3),\n",
       " (0.7547, 0.112, 0.72, 3),\n",
       " (0.7547, 0.111, 0.72, 3),\n",
       " (0.7547, 0.11, 0.72, 3),\n",
       " (0.7547, 0.109, 0.72, 3),\n",
       " (0.7547, 0.108, 0.72, 3),\n",
       " (0.7547, 0.107, 0.72, 3),\n",
       " (0.7547, 0.096, 0.74, 3),\n",
       " (0.7547, 0.095, 0.74, 3),\n",
       " (0.7547, 0.094, 0.74, 3),\n",
       " (0.7547, 0.093, 0.74, 3),\n",
       " (0.7547, 0.092, 0.74, 3),\n",
       " (0.7547, 0.091, 0.74, 3),\n",
       " (0.7547, 0.09, 0.74, 3),\n",
       " (0.7547, 0.089, 0.74, 3),\n",
       " (0.7547, 0.086, 0.72, 3),\n",
       " (0.7547, 0.085, 0.72, 3),\n",
       " (0.7547, 0.079, 0.74, 3),\n",
       " (0.7547, 0.076, 0.74, 3),\n",
       " (0.7547, 0.074, 0.74, 3),\n",
       " (0.7547, 0.073, 0.73, 3),\n",
       " (0.7547, 0.072, 0.74, 3),\n",
       " (0.7547, 0.072, 0.74, 1),\n",
       " (0.7547, 0.072, 0.72, 1),\n",
       " (0.7547, 0.069, 0.74, 1),\n",
       " (0.7547, 0.069, 0.72, 1),\n",
       " (0.7547, 0.068, 0.74, 1),\n",
       " (0.7547, 0.067, 0.74, 3),\n",
       " (0.7547, 0.066, 0.74, 3),\n",
       " (0.7547, 0.066, 0.73, 1),\n",
       " (0.7547, 0.065, 0.74, 3),\n",
       " (0.7547, 0.064, 0.74, 3),\n",
       " (0.7547, 0.064, 0.73, 1),\n",
       " (0.7547, 0.062, 0.73, 3),\n",
       " (0.7547, 0.062, 0.73, 1),\n",
       " (0.7547, 0.061, 0.73, 3),\n",
       " (0.7547, 0.061, 0.73, 1),\n",
       " (0.7547, 0.059, 0.73, 1),\n",
       " (0.7547, 0.053, 0.73, 3),\n",
       " (0.7547, 0.052, 0.74, 3),\n",
       " (0.7547, 0.049, 0.74, 3),\n",
       " (0.7547, 0.049, 0.73, 3),\n",
       " (0.7547, 0.046, 0.73, 3),\n",
       " (0.7547, 0.034, 0.73, 3),\n",
       " (0.7547, 0.033, 0.74, 3),\n",
       " (0.7547, 0.032, 0.73, 3),\n",
       " (0.75465, 0.148, 0.72, 2),\n",
       " (0.75465, 0.147, 0.72, 2),\n",
       " (0.75465, 0.12, 0.72, 3),\n",
       " (0.75465, 0.116, 0.72, 3),\n",
       " (0.75465, 0.106, 0.72, 3),\n",
       " (0.75465, 0.08, 0.74, 3),\n",
       " (0.75465, 0.078, 0.73, 3),\n",
       " (0.75465, 0.077, 0.73, 3),\n",
       " (0.75465, 0.075, 0.73, 3),\n",
       " (0.75465, 0.074, 0.73, 3),\n",
       " (0.75465, 0.073, 0.74, 1),\n",
       " (0.75465, 0.073, 0.72, 1),\n",
       " (0.75465, 0.072, 0.73, 3),\n",
       " (0.75465, 0.069, 0.74, 3),\n",
       " (0.75465, 0.068, 0.74, 3),\n",
       " (0.75465, 0.067, 0.73, 3),\n",
       " (0.75465, 0.066, 0.73, 3),\n",
       " (0.75465, 0.065, 0.73, 3),\n",
       " (0.75465, 0.064, 0.73, 3),\n",
       " (0.75465, 0.06, 0.73, 1),\n",
       " (0.75465, 0.058, 0.73, 1),\n",
       " (0.75465, 0.052, 0.73, 3),\n",
       " (0.75465, 0.051, 0.74, 3),\n",
       " (0.75465, 0.05, 0.74, 3),\n",
       " (0.75465, 0.048, 0.74, 3),\n",
       " (0.75465, 0.048, 0.73, 3),\n",
       " (0.75465, 0.047, 0.74, 3),\n",
       " (0.75465, 0.047, 0.73, 3),\n",
       " (0.75465, 0.045, 0.74, 3),\n",
       " (0.75465, 0.029, 0.73, 3),\n",
       " (0.7546, 0.121, 0.72, 3),\n",
       " (0.7546, 0.119, 0.74, 3),\n",
       " (0.7546, 0.118, 0.74, 3),\n",
       " (0.7546, 0.117, 0.74, 3),\n",
       " (0.7546, 0.115, 0.74, 3),\n",
       " (0.7546, 0.114, 0.74, 3),\n",
       " (0.7546, 0.113, 0.74, 3),\n",
       " (0.7546, 0.112, 0.74, 3),\n",
       " (0.7546, 0.111, 0.74, 3),\n",
       " (0.7546, 0.11, 0.74, 3),\n",
       " (0.7546, 0.109, 0.74, 3),\n",
       " (0.7546, 0.108, 0.74, 3),\n",
       " (0.7546, 0.105, 0.72, 3),\n",
       " (0.7546, 0.104, 0.72, 3),\n",
       " (0.7546, 0.103, 0.72, 3),\n",
       " (0.7546, 0.102, 0.72, 3),\n",
       " (0.7546, 0.101, 0.72, 3),\n",
       " (0.7546, 0.1, 0.72, 3),\n",
       " (0.7546, 0.099, 0.72, 3),\n",
       " (0.7546, 0.098, 0.74, 3),\n",
       " (0.7546, 0.097, 0.74, 3),\n",
       " (0.7546, 0.088, 0.74, 3),\n",
       " (0.7546, 0.081, 0.74, 3),\n",
       " (0.7546, 0.079, 0.73, 3),\n",
       " (0.7546, 0.076, 0.73, 3),\n",
       " (0.7546, 0.075, 0.74, 1),\n",
       " (0.7546, 0.075, 0.72, 1),\n",
       " (0.7546, 0.074, 0.74, 1),\n",
       " (0.7546, 0.074, 0.72, 1),\n",
       " (0.7546, 0.071, 0.74, 3),\n",
       " (0.7546, 0.071, 0.73, 1),\n",
       " (0.7546, 0.07, 0.74, 3),\n",
       " (0.7546, 0.069, 0.73, 3),\n",
       " (0.7546, 0.068, 0.73, 3),\n",
       " (0.7546, 0.051, 0.73, 3),\n",
       " (0.7546, 0.05, 0.73, 3),\n",
       " (0.7546, 0.047, 0.74, 1),\n",
       " (0.7546, 0.045, 0.73, 3),\n",
       " (0.7546, 0.043, 0.72, 1),\n",
       " (0.7546, 0.039, 0.72, 1),\n",
       " (0.7546, 0.035, 0.73, 3),\n",
       " (0.7546, 0.034, 0.74, 3),\n",
       " (0.7546, 0.032, 0.74, 3),\n",
       " (0.7546, 0.03, 0.73, 3),\n",
       " (0.7546, 0.028, 0.73, 3),\n",
       " (0.75455, 0.149, 0.72, 2),\n",
       " (0.75455, 0.148, 0.74, 2),\n",
       " (0.75455, 0.147, 0.74, 2),\n",
       " (0.75455, 0.146, 0.72, 2),\n",
       " (0.75455, 0.122, 0.72, 3),\n",
       " (0.75455, 0.12, 0.74, 3),\n",
       " (0.75455, 0.116, 0.74, 3),\n",
       " (0.75455, 0.107, 0.74, 3),\n",
       " (0.75455, 0.087, 0.74, 3),\n",
       " (0.75455, 0.084, 0.74, 3),\n",
       " (0.75455, 0.083, 0.74, 3),\n",
       " (0.75455, 0.082, 0.74, 3),\n",
       " (0.75455, 0.08, 0.73, 3),\n",
       " (0.75455, 0.071, 0.73, 3),\n",
       " (0.75455, 0.07, 0.73, 3),\n",
       " (0.75455, 0.067, 0.73, 1),\n",
       " (0.75455, 0.046, 0.74, 1),\n",
       " (0.75455, 0.042, 0.72, 1),\n",
       " (0.75455, 0.041, 0.72, 1),\n",
       " (0.75455, 0.035, 0.74, 3),\n",
       " (0.75455, 0.035, 0.72, 1),\n",
       " (0.75455, 0.031, 0.73, 3),\n",
       " (0.75455, 0.029, 0.74, 3),\n",
       " (0.75455, 0.025, 0.72, 3),\n",
       " (0.7545, 0.145, 0.72, 2),\n",
       " (0.7545, 0.14, 0.72, 2),\n",
       " (0.7545, 0.122, 0.74, 3),\n",
       " (0.7545, 0.121, 0.74, 3),\n",
       " (0.7545, 0.106, 0.74, 3),\n",
       " (0.7545, 0.096, 0.73, 3),\n",
       " (0.7545, 0.095, 0.73, 3),\n",
       " (0.7545, 0.094, 0.73, 3),\n",
       " (0.7545, 0.093, 0.73, 3),\n",
       " (0.7545, 0.092, 0.73, 3),\n",
       " (0.7545, 0.091, 0.73, 3),\n",
       " (0.7545, 0.09, 0.73, 3),\n",
       " (0.7545, 0.089, 0.73, 3),\n",
       " (0.7545, 0.086, 0.74, 3),\n",
       " (0.7545, 0.085, 0.74, 3),\n",
       " (0.7545, 0.076, 0.74, 1),\n",
       " (0.7545, 0.076, 0.72, 1),\n",
       " (0.7545, 0.07, 0.73, 1),\n",
       " (0.7545, 0.047, 0.73, 1),\n",
       " (0.7545, 0.044, 0.74, 3),\n",
       " (0.7545, 0.042, 0.74, 3),\n",
       " (0.7545, 0.042, 0.73, 3),\n",
       " (0.7545, 0.039, 0.74, 3),\n",
       " (0.7545, 0.039, 0.73, 3),\n",
       " (0.7545, 0.038, 0.72, 1),\n",
       " (0.7545, 0.036, 0.73, 3),\n",
       " (0.7545, 0.032, 0.74, 1),\n",
       " (0.7545, 0.03, 0.74, 3),\n",
       " (0.7545, 0.028, 0.74, 3),\n",
       " (0.75445, 0.149, 0.74, 2),\n",
       " (0.75445, 0.146, 0.74, 2),\n",
       " (0.75445, 0.144, 0.72, 2),\n",
       " (0.75445, 0.143, 0.72, 2),\n",
       " (0.75445, 0.142, 0.72, 2),\n",
       " (0.75445, 0.141, 0.72, 3),\n",
       " (0.75445, 0.141, 0.72, 2),\n",
       " (0.75445, 0.133, 0.72, 2),\n",
       " (0.75445, 0.125, 0.72, 2),\n",
       " (0.75445, 0.124, 0.72, 2),\n",
       " (0.75445, 0.105, 0.74, 3),\n",
       " (0.75445, 0.104, 0.74, 3),\n",
       " (0.75445, 0.103, 0.74, 3),\n",
       " (0.75445, 0.081, 0.73, 3),\n",
       " (0.75445, 0.072, 0.73, 1),\n",
       " (0.75445, 0.069, 0.73, 1),\n",
       " (0.75445, 0.068, 0.73, 1),\n",
       " (0.75445, 0.046, 0.73, 1),\n",
       " (0.75445, 0.044, 0.73, 3),\n",
       " (0.75445, 0.043, 0.74, 3),\n",
       " (0.75445, 0.043, 0.73, 3),\n",
       " (0.75445, 0.041, 0.74, 3),\n",
       " (0.75445, 0.041, 0.73, 3),\n",
       " (0.75445, 0.037, 0.72, 1),\n",
       " (0.75445, 0.036, 0.74, 3),\n",
       " (0.75445, 0.032, 0.73, 1),\n",
       " (0.75445, 0.031, 0.74, 3),\n",
       " (0.75445, 0.03, 0.72, 1),\n",
       " (0.75445, 0.029, 0.72, 1),\n",
       " (0.7544, 0.145, 0.74, 2),\n",
       " (0.7544, 0.143, 0.74, 3),\n",
       " (0.7544, 0.143, 0.72, 3),\n",
       " (0.7544, 0.142, 0.74, 3),\n",
       " (0.7544, 0.142, 0.72, 3),\n",
       " (0.7544, 0.141, 0.74, 3),\n",
       " (0.7544, 0.14, 0.74, 2),\n",
       " (0.7544, 0.14, 0.72, 3),\n",
       " (0.7544, 0.139, 0.72, 3),\n",
       " (0.7544, 0.139, 0.72, 2),\n",
       " (0.7544, 0.138, 0.72, 3),\n",
       " (0.7544, 0.137, 0.72, 3),\n",
       " (0.7544, 0.136, 0.72, 3),\n",
       " (0.7544, 0.136, 0.72, 2),\n",
       " (0.7544, 0.135, 0.72, 3),\n",
       " (0.7544, 0.135, 0.72, 2),\n",
       " (0.7544, 0.134, 0.72, 3),\n",
       " (0.7544, 0.134, 0.72, 2),\n",
       " (0.7544, 0.133, 0.72, 3),\n",
       " (0.7544, 0.132, 0.72, 3),\n",
       " (0.7544, 0.132, 0.72, 2),\n",
       " (0.7544, 0.131, 0.72, 3),\n",
       " (0.7544, 0.13, 0.72, 3),\n",
       " (0.7544, 0.129, 0.72, 3),\n",
       " (0.7544, 0.128, 0.72, 3),\n",
       " (0.7544, 0.127, 0.72, 3),\n",
       " (0.7544, 0.126, 0.72, 3),\n",
       " (0.7544, 0.126, 0.72, 2),\n",
       " (0.7544, 0.123, 0.72, 3),\n",
       " (0.7544, 0.102, 0.74, 3),\n",
       " (0.7544, 0.101, 0.74, 3),\n",
       " (0.7544, 0.1, 0.74, 3),\n",
       " (0.7544, 0.099, 0.74, 3),\n",
       " (0.7544, 0.098, 0.73, 3),\n",
       " (0.7544, 0.097, 0.73, 3),\n",
       " (0.7544, 0.088, 0.73, 3),\n",
       " (0.7544, 0.082, 0.73, 3),\n",
       " (0.7544, 0.077, 0.74, 1),\n",
       " (0.7544, 0.077, 0.72, 1),\n",
       " (0.7544, 0.073, 0.73, 1),\n",
       " (0.7544, 0.045, 0.74, 1),\n",
       " (0.7544, 0.034, 0.72, 1),\n",
       " (0.7544, 0.026, 0.73, 3),\n",
       " (0.75435, 0.149, 0.74, 3),\n",
       " (0.75435, 0.149, 0.72, 3),\n",
       " (0.75435, 0.148, 0.74, 3),\n",
       " (0.75435, 0.148, 0.72, 3),\n",
       " (0.75435, 0.147, 0.74, 3),\n",
       " (0.75435, 0.147, 0.72, 3),\n",
       " (0.75435, 0.146, 0.74, 3),\n",
       " (0.75435, 0.146, 0.72, 3),\n",
       " (0.75435, 0.145, 0.74, 3),\n",
       " (0.75435, 0.145, 0.72, 3),\n",
       " (0.75435, 0.144, 0.74, 3),\n",
       " (0.75435, 0.144, 0.74, 2),\n",
       " (0.75435, 0.144, 0.72, 3),\n",
       " (0.75435, 0.143, 0.74, 2),\n",
       " (0.75435, 0.142, 0.74, 2),\n",
       " (0.75435, 0.141, 0.74, 2),\n",
       " (0.75435, 0.14, 0.74, 3),\n",
       " (0.75435, 0.139, 0.74, 3),\n",
       " (0.75435, 0.138, 0.74, 3),\n",
       " (0.75435, 0.138, 0.72, 2),\n",
       " (0.75435, 0.137, 0.74, 3),\n",
       " (0.75435, 0.137, 0.72, 2),\n",
       " (0.75435, 0.136, 0.74, 3),\n",
       " (0.75435, 0.135, 0.74, 3),\n",
       " (0.75435, 0.134, 0.74, 3),\n",
       " (0.75435, 0.133, 0.74, 3),\n",
       " (0.75435, 0.133, 0.74, 2),\n",
       " (0.75435, 0.132, 0.74, 3),\n",
       " (0.75435, 0.131, 0.74, 3),\n",
       " (0.75435, 0.13, 0.74, 3),\n",
       " (0.75435, 0.129, 0.74, 3),\n",
       " (0.75435, 0.129, 0.72, 2),\n",
       " (0.75435, 0.128, 0.74, 3),\n",
       " (0.75435, 0.128, 0.72, 2),\n",
       " (0.75435, 0.127, 0.74, 3),\n",
       " (0.75435, 0.126, 0.74, 3),\n",
       " (0.75435, 0.125, 0.72, 3),\n",
       " (0.75435, 0.124, 0.72, 3),\n",
       " (0.75435, 0.123, 0.74, 3),\n",
       " (0.75435, 0.123, 0.72, 2),\n",
       " (0.75435, 0.121, 0.72, 2),\n",
       " (0.75435, 0.12, 0.72, 2),\n",
       " (0.75435, 0.119, 0.73, 3),\n",
       " (0.75435, 0.118, 0.73, 3),\n",
       " (0.75435, 0.117, 0.73, 3),\n",
       " (0.75435, 0.115, 0.73, 3),\n",
       " (0.75435, 0.114, 0.73, 3),\n",
       " (0.75435, 0.113, 0.73, 3),\n",
       " (0.75435, 0.112, 0.73, 3),\n",
       " (0.75435, 0.111, 0.73, 3),\n",
       " (0.75435, 0.11, 0.73, 3),\n",
       " (0.75435, 0.109, 0.73, 3),\n",
       " (0.75435, 0.108, 0.73, 3),\n",
       " (0.75435, 0.107, 0.73, 3),\n",
       " (0.75435, 0.087, 0.73, 3),\n",
       " (0.75435, 0.084, 0.73, 3),\n",
       " (0.75435, 0.083, 0.73, 3),\n",
       " (0.75435, 0.079, 0.74, 1),\n",
       " (0.75435, 0.079, 0.72, 1),\n",
       " (0.75435, 0.078, 0.74, 1),\n",
       " (0.75435, 0.078, 0.72, 1),\n",
       " (0.75435, 0.075, 0.73, 1),\n",
       " (0.75435, 0.074, 0.73, 1),\n",
       " (0.75435, 0.045, 0.73, 1),\n",
       " (0.75435, 0.044, 0.74, 1),\n",
       " (0.75435, 0.04, 0.74, 3),\n",
       " (0.75435, 0.04, 0.73, 3),\n",
       " (0.75435, 0.031, 0.74, 1),\n",
       " (0.75435, 0.028, 0.72, 1),\n",
       " (0.75435, 0.027, 0.73, 3),\n",
       " (0.7543, 0.148, 0.73, 2),\n",
       " (0.7543, 0.147, 0.73, 2),\n",
       " (0.7543, 0.139, 0.74, 2),\n",
       " (0.7543, 0.136, 0.74, 2),\n",
       " (0.7543, 0.135, 0.74, 2),\n",
       " (0.7543, 0.134, 0.74, 2),\n",
       " (0.7543, 0.132, 0.74, 2),\n",
       " (0.7543, 0.131, 0.72, 2),\n",
       " (0.7543, 0.127, 0.72, 2),\n",
       " (0.7543, 0.125, 0.74, 3),\n",
       " (0.7543, 0.125, 0.74, 2),\n",
       " (0.7543, 0.124, 0.74, 3),\n",
       " (0.7543, 0.124, 0.74, 2),\n",
       " (0.7543, 0.12, 0.73, 3),\n",
       " (0.7543, 0.116, 0.73, 3),\n",
       " (0.7543, 0.106, 0.73, 3),\n",
       " (0.7543, 0.086, 0.73, 3),\n",
       " (0.7543, 0.085, 0.73, 3),\n",
       " (0.7543, 0.084, 0.74, 1),\n",
       " (0.7543, 0.084, 0.72, 1),\n",
       " (0.7543, 0.083, 0.74, 1),\n",
       " (0.7543, 0.083, 0.72, 1),\n",
       " (0.7543, 0.081, 0.74, 1),\n",
       " (0.7543, 0.081, 0.72, 1),\n",
       " (0.7543, 0.08, 0.74, 1),\n",
       " (0.7543, 0.08, 0.72, 1),\n",
       " (0.7543, 0.044, 0.73, 1),\n",
       " (0.7543, 0.038, 0.74, 3),\n",
       " (0.7543, 0.038, 0.73, 3),\n",
       " (0.7543, 0.037, 0.73, 3),\n",
       " (0.7543, 0.031, 0.73, 1),\n",
       " (0.7543, 0.026, 0.74, 3),\n",
       " (0.7543, 0.022, 0.72, 3),\n",
       " (0.75425, 0.138, 0.74, 2),\n",
       " (0.75425, 0.137, 0.74, 2),\n",
       " (0.75425, 0.13, 0.72, 2),\n",
       " (0.75425, 0.126, 0.74, 2),\n",
       " (0.75425, 0.122, 0.73, 3),\n",
       " (0.75425, 0.122, 0.72, 2),\n",
       " (0.75425, 0.121, 0.73, 3),\n",
       " (0.75425, 0.119, 0.72, 2),\n",
       " (0.75425, 0.105, 0.73, 3),\n",
       " (0.75425, 0.104, 0.73, 3),\n",
       " (0.75425, 0.104, 0.72, 2),\n",
       " (0.75425, 0.103, 0.73, 3),\n",
       " (0.75425, 0.103, 0.72, 2),\n",
       " (0.75425, 0.082, 0.74, 1),\n",
       " (0.75425, 0.082, 0.72, 1),\n",
       " (0.75425, 0.076, 0.73, 1),\n",
       " (0.75425, 0.037, 0.74, 3),\n",
       " (0.75425, 0.027, 0.74, 3),\n",
       " (0.75425, 0.026, 0.72, 1),\n",
       " (0.75425, 0.024, 0.72, 3),\n",
       " (0.75425, 0.021, 0.72, 3),\n",
       " (0.7542, 0.149, 0.73, 2),\n",
       " (0.7542, 0.146, 0.73, 2),\n",
       " (0.7542, 0.131, 0.74, 2),\n",
       " (0.7542, 0.129, 0.74, 2),\n",
       " (0.7542, 0.128, 0.74, 2),\n",
       " (0.7542, 0.123, 0.74, 2),\n",
       " (0.7542, 0.121, 0.74, 2),\n",
       " (0.7542, 0.12, 0.74, 2),\n",
       " (0.7542, 0.115, 0.72, 2),\n",
       " (0.7542, 0.111, 0.72, 2),\n",
       " (0.7542, 0.109, 0.72, 2),\n",
       " (0.7542, 0.102, 0.73, 3),\n",
       " (0.7542, 0.101, 0.73, 3),\n",
       " (0.7542, 0.1, 0.73, 3),\n",
       " (0.7542, 0.099, 0.73, 3),\n",
       " (0.7542, 0.043, 0.74, 1),\n",
       " (0.7542, 0.04, 0.74, 1),\n",
       " (0.7542, 0.04, 0.73, 1),\n",
       " (0.7542, 0.033, 0.74, 1),\n",
       " (0.7542, 0.023, 0.72, 3),\n",
       " (0.7542, 0.019, 0.72, 3),\n",
       " (0.7542, 0.017, 0.72, 3),\n",
       " (0.7542, 0.016, 0.72, 3),\n",
       " (0.75415, 0.145, 0.73, 2),\n",
       " (0.75415, 0.143, 0.73, 3),\n",
       " (0.75415, 0.142, 0.73, 3),\n",
       " (0.75415, 0.141, 0.73, 3),\n",
       " (0.75415, 0.14, 0.73, 2),\n",
       " (0.75415, 0.127, 0.74, 2),\n",
       " (0.75415, 0.118, 0.72, 2),\n",
       " (0.75415, 0.117, 0.72, 2),\n",
       " (0.75415, 0.114, 0.72, 2),\n",
       " (0.75415, 0.112, 0.72, 2),\n",
       " (0.75415, 0.111, 0.74, 1),\n",
       " (0.75415, 0.111, 0.72, 1),\n",
       " (0.75415, 0.11, 0.74, 1),\n",
       " (0.75415, 0.11, 0.72, 1),\n",
       " (0.75415, 0.109, 0.74, 1),\n",
       " (0.75415, 0.109, 0.72, 1),\n",
       " (0.75415, 0.108, 0.74, 1),\n",
       " (0.75415, 0.108, 0.72, 1),\n",
       " (0.75415, 0.107, 0.74, 1),\n",
       " (0.75415, 0.107, 0.72, 1),\n",
       " (0.75415, 0.085, 0.74, 1),\n",
       " (0.75415, 0.085, 0.72, 1),\n",
       " (0.75415, 0.077, 0.73, 1),\n",
       " (0.75415, 0.043, 0.73, 1),\n",
       " (0.75415, 0.042, 0.74, 1),\n",
       " (0.75415, 0.033, 0.73, 1),\n",
       " (0.75415, 0.027, 0.72, 1),\n",
       " (0.75415, 0.018, 0.72, 3),\n",
       " (0.7541, 0.149, 0.73, 3),\n",
       " (0.7541, 0.148, 0.73, 3),\n",
       " (0.7541, 0.147, 0.73, 3),\n",
       " (0.7541, 0.146, 0.73, 3),\n",
       " (0.7541, 0.145, 0.73, 3),\n",
       " (0.7541, 0.144, 0.73, 3),\n",
       " (0.7541, 0.144, 0.73, 2),\n",
       " (0.7541, 0.143, 0.73, 2),\n",
       " (0.7541, 0.142, 0.73, 2),\n",
       " (0.7541, 0.141, 0.73, 2),\n",
       " (0.7541, 0.14, 0.73, 3),\n",
       " (0.7541, 0.139, 0.73, 3),\n",
       " (0.7541, 0.138, 0.73, 3),\n",
       " (0.7541, 0.137, 0.73, 3),\n",
       " (0.7541, 0.136, 0.73, 3),\n",
       " (0.7541, 0.135, 0.73, 3),\n",
       " (0.7541, 0.134, 0.73, 3),\n",
       " (0.7541, 0.133, 0.73, 3),\n",
       " (0.7541, 0.133, 0.73, 2),\n",
       " (0.7541, 0.132, 0.73, 3),\n",
       " (0.7541, 0.131, 0.73, 3),\n",
       " (0.7541, 0.13, 0.74, 2),\n",
       " (0.7541, 0.13, 0.73, 3),\n",
       " (0.7541, 0.129, 0.73, 3),\n",
       " (0.7541, 0.128, 0.73, 3),\n",
       " (0.7541, 0.127, 0.73, 3),\n",
       " (0.7541, 0.126, 0.73, 3),\n",
       " (0.7541, 0.125, 0.73, 2),\n",
       " (0.7541, 0.124, 0.73, 2),\n",
       " (0.7541, 0.123, 0.73, 3),\n",
       " (0.7541, 0.122, 0.74, 2),\n",
       " (0.7541, 0.119, 0.74, 2),\n",
       " (0.7541, 0.116, 0.72, 2),\n",
       " (0.7541, 0.115, 0.74, 1),\n",
       " (0.7541, 0.115, 0.72, 1),\n",
       " (0.7541, 0.114, 0.74, 1),\n",
       " (0.7541, 0.114, 0.72, 1),\n",
       " (0.7541, 0.112, 0.74, 1),\n",
       " (0.7541, 0.112, 0.72, 1),\n",
       " (0.7541, 0.107, 0.72, 2),\n",
       " (0.7541, 0.106, 0.74, 1),\n",
       " (0.7541, 0.106, 0.72, 1),\n",
       " (0.7541, 0.105, 0.74, 1),\n",
       " (0.7541, 0.105, 0.72, 1),\n",
       " (0.7541, 0.104, 0.74, 1),\n",
       " (0.7541, 0.104, 0.72, 1),\n",
       " (0.7541, 0.101, 0.72, 2),\n",
       " (0.7541, 0.09, 0.74, 1),\n",
       " (0.7541, 0.09, 0.72, 1),\n",
       " (0.7541, 0.089, 0.74, 1),\n",
       " (0.7541, 0.089, 0.72, 1),\n",
       " (0.7541, 0.086, 0.74, 1),\n",
       " (0.7541, 0.086, 0.72, 1),\n",
       " (0.7541, 0.079, 0.73, 1),\n",
       " (0.7541, 0.078, 0.73, 1),\n",
       " (0.7541, 0.042, 0.73, 1),\n",
       " (0.7541, 0.036, 0.74, 1),\n",
       " (0.7541, 0.025, 0.73, 3),\n",
       " (0.7541, 0.015, 0.72, 3),\n",
       " (0.75405, 0.139, 0.73, 2),\n",
       " (0.75405, 0.136, 0.73, 2),\n",
       " (0.75405, 0.135, 0.73, 2),\n",
       " (0.75405, 0.134, 0.73, 2),\n",
       " (0.75405, 0.132, 0.73, 2),\n",
       " (0.75405, 0.126, 0.73, 2),\n",
       " (0.75405, 0.125, 0.73, 3),\n",
       " (0.75405, 0.124, 0.73, 3),\n",
       " (0.75405, 0.12, 0.74, 1),\n",
       " (0.75405, 0.12, 0.72, 1),\n",
       " (0.75405, 0.119, 0.74, 1),\n",
       " (0.75405, 0.119, 0.72, 1),\n",
       " (0.75405, 0.118, 0.74, 1),\n",
       " (0.75405, 0.118, 0.72, 1),\n",
       " (0.75405, 0.117, 0.74, 1),\n",
       " (0.75405, 0.117, 0.72, 1),\n",
       " (0.75405, 0.116, 0.74, 1),\n",
       " (0.75405, 0.116, 0.72, 1),\n",
       " (0.75405, 0.113, 0.74, 1),\n",
       " (0.75405, 0.113, 0.72, 2),\n",
       " (0.75405, 0.113, 0.72, 1),\n",
       " (0.75405, 0.11, 0.72, 2),\n",
       " (0.75405, 0.108, 0.72, 2),\n",
       " (0.75405, 0.105, 0.72, 2),\n",
       " (0.75405, 0.103, 0.74, 1),\n",
       " (0.75405, 0.103, 0.72, 1),\n",
       " (0.75405, 0.102, 0.74, 1),\n",
       " (0.75405, 0.102, 0.72, 2),\n",
       " (0.75405, 0.102, 0.72, 1),\n",
       " (0.75405, 0.097, 0.74, 1),\n",
       " (0.75405, 0.097, 0.72, 1),\n",
       " (0.75405, 0.096, 0.74, 1),\n",
       " (0.75405, 0.096, 0.72, 1),\n",
       " (0.75405, 0.095, 0.74, 1),\n",
       " (0.75405, 0.095, 0.72, 1),\n",
       " (0.75405, 0.094, 0.74, 1),\n",
       " (0.75405, 0.094, 0.72, 1),\n",
       " (0.75405, 0.093, 0.74, 1),\n",
       " (0.75405, 0.093, 0.72, 1),\n",
       " (0.75405, 0.092, 0.74, 1),\n",
       " (0.75405, 0.092, 0.72, 1),\n",
       " (0.75405, 0.091, 0.74, 1),\n",
       " (0.75405, 0.091, 0.72, 1),\n",
       " (0.75405, 0.088, 0.74, 1),\n",
       " (0.75405, 0.088, 0.72, 1),\n",
       " (0.75405, 0.087, 0.74, 1),\n",
       " (0.75405, 0.087, 0.72, 1),\n",
       " (0.75405, 0.084, 0.73, 1),\n",
       " (0.75405, 0.083, 0.73, 1),\n",
       " (0.75405, 0.081, 0.73, 1),\n",
       " (0.75405, 0.08, 0.73, 1),\n",
       " (0.75405, 0.041, 0.74, 1),\n",
       " (0.75405, 0.039, 0.74, 1),\n",
       " (0.75405, 0.036, 0.73, 1),\n",
       " (0.75405, 0.02, 0.72, 3),\n",
       " (0.754, 0.148, 0.74, 1),\n",
       " (0.754, 0.148, 0.72, 1),\n",
       " (0.754, 0.147, 0.74, 1),\n",
       " (0.754, 0.147, 0.72, 1),\n",
       " (0.754, 0.146, 0.74, 1),\n",
       " (0.754, 0.146, 0.72, 1),\n",
       " (0.754, 0.145, 0.74, 1),\n",
       " (0.754, 0.145, 0.72, 1),\n",
       " (0.754, 0.144, 0.74, 1),\n",
       " (0.754, 0.144, 0.72, 1),\n",
       " (0.754, 0.143, 0.74, 1),\n",
       " (0.754, 0.143, 0.72, 1),\n",
       " (0.754, 0.142, 0.74, 1),\n",
       " (0.754, 0.142, 0.72, 1),\n",
       " (0.754, 0.141, 0.74, 1),\n",
       " (0.754, 0.141, 0.72, 1),\n",
       " (0.754, 0.14, 0.74, 1),\n",
       " (0.754, 0.14, 0.72, 1),\n",
       " (0.754, 0.139, 0.74, 1),\n",
       " (0.754, 0.139, 0.72, 1),\n",
       " (0.754, 0.138, 0.74, 1),\n",
       " (0.754, 0.138, 0.73, 2),\n",
       " (0.754, 0.138, 0.72, 1),\n",
       " (0.754, 0.137, 0.74, 1),\n",
       " (0.754, 0.137, 0.73, 2),\n",
       " (0.754, 0.137, 0.72, 1),\n",
       " (0.754, 0.136, 0.74, 1),\n",
       " (0.754, 0.136, 0.72, 1),\n",
       " (0.754, 0.135, 0.74, 1),\n",
       " (0.754, 0.135, 0.72, 1),\n",
       " (0.754, 0.134, 0.74, 1),\n",
       " (0.754, 0.134, 0.72, 1),\n",
       " (0.754, 0.133, 0.74, 1),\n",
       " (0.754, 0.133, 0.72, 1),\n",
       " (0.754, 0.129, 0.73, 2),\n",
       " (0.754, 0.128, 0.73, 2),\n",
       " (0.754, 0.123, 0.74, 1),\n",
       " (0.754, 0.123, 0.73, 2),\n",
       " (0.754, 0.123, 0.72, 1),\n",
       " (0.754, 0.122, 0.74, 1),\n",
       " (0.754, 0.122, 0.72, 1),\n",
       " (0.754, 0.121, 0.74, 1),\n",
       " (0.754, 0.121, 0.73, 2),\n",
       " (0.754, 0.121, 0.72, 1),\n",
       " (0.754, 0.12, 0.73, 2),\n",
       " (0.754, 0.118, 0.74, 2),\n",
       " (0.754, 0.117, 0.74, 2),\n",
       " (0.754, 0.115, 0.74, 2),\n",
       " (0.754, 0.106, 0.72, 2),\n",
       " (0.754, 0.099, 0.74, 1),\n",
       " (0.754, 0.099, 0.72, 1),\n",
       " (0.754, 0.098, 0.74, 1),\n",
       " (0.754, 0.098, 0.72, 1),\n",
       " (0.754, 0.082, 0.73, 1),\n",
       " (0.754, 0.041, 0.73, 1),\n",
       " (0.754, 0.039, 0.73, 1),\n",
       " (0.754, 0.032, 0.71, 1),\n",
       " (0.754, 0.025, 0.74, 3),\n",
       " (0.75395, 0.149, 0.74, 1),\n",
       " (0.75395, 0.149, 0.72, 1),\n",
       " (0.75395, 0.132, 0.74, 1),\n",
       " (0.75395, 0.132, 0.72, 1),\n",
       " (0.75395, 0.131, 0.74, 1),\n",
       " (0.75395, 0.131, 0.73, 2),\n",
       " (0.75395, 0.131, 0.72, 1),\n",
       " (0.75395, 0.13, 0.74, 1),\n",
       " (0.75395, 0.13, 0.72, 1),\n",
       " (0.75395, 0.129, 0.74, 1),\n",
       " (0.75395, 0.129, 0.72, 1),\n",
       " (0.75395, 0.128, 0.74, 1),\n",
       " (0.75395, 0.128, 0.72, 1),\n",
       " (0.75395, 0.127, 0.73, 2),\n",
       " (0.75395, 0.124, 0.74, 1),\n",
       " (0.75395, 0.124, 0.72, 1),\n",
       " (0.75395, 0.114, 0.74, 2),\n",
       " (0.75395, 0.111, 0.74, 2),\n",
       " (0.75395, 0.101, 0.74, 1),\n",
       " (0.75395, 0.101, 0.72, 1),\n",
       " (0.75395, 0.1, 0.74, 1),\n",
       " (0.75395, 0.1, 0.72, 2),\n",
       " (0.75395, 0.1, 0.72, 1),\n",
       " (0.75395, 0.098, 0.72, 2),\n",
       " (0.75395, 0.097, 0.72, 2),\n",
       " (0.75395, 0.05, 0.71, 1),\n",
       " (0.75395, 0.035, 0.74, 1),\n",
       " (0.7539, 0.13, 0.73, 2),\n",
       " (0.7539, 0.127, 0.74, 1),\n",
       " (0.7539, 0.127, 0.72, 1),\n",
       " (0.7539, 0.126, 0.74, 1),\n",
       " (0.7539, 0.126, 0.72, 1),\n",
       " (0.7539, 0.125, 0.74, 1),\n",
       " (0.7539, 0.125, 0.72, 1),\n",
       " (0.7539, 0.122, 0.73, 2),\n",
       " (0.7539, 0.119, 0.73, 2),\n",
       " (0.7539, 0.116, 0.74, 2),\n",
       " (0.7539, 0.112, 0.74, 2),\n",
       " (0.7539, 0.111, 0.73, 1),\n",
       " (0.7539, 0.11, 0.73, 1),\n",
       " (0.7539, 0.109, 0.74, 2),\n",
       " (0.7539, 0.109, 0.73, 1),\n",
       " (0.7539, 0.108, 0.73, 1),\n",
       " (0.7539, 0.107, 0.73, 1),\n",
       " (0.7539, 0.099, 0.72, 2),\n",
       " (0.7539, 0.085, 0.73, 1),\n",
       " (0.7539, 0.038, 0.74, 1),\n",
       " (0.7539, 0.033, 0.71, 3),\n",
       " (0.75385, 0.115, 0.73, 2),\n",
       " (0.75385, 0.115, 0.73, 1),\n",
       " (0.75385, 0.114, 0.73, 1),\n",
       " (0.75385, 0.113, 0.74, 2),\n",
       " (0.75385, 0.112, 0.73, 1),\n",
       " (0.75385, 0.111, 0.73, 2),\n",
       " (0.75385, 0.106, 0.73, 1),\n",
       " (0.75385, 0.105, 0.73, 1),\n",
       " (0.75385, 0.104, 0.74, 2),\n",
       " (0.75385, 0.104, 0.73, 1),\n",
       " (0.75385, 0.103, 0.74, 2),\n",
       " (0.75385, 0.096, 0.72, 2),\n",
       " (0.75385, 0.09, 0.73, 1),\n",
       " (0.75385, 0.089, 0.73, 1),\n",
       " (0.75385, 0.086, 0.73, 1),\n",
       " (0.75385, 0.051, 0.71, 1),\n",
       " (0.75385, 0.049, 0.71, 1),\n",
       " (0.75385, 0.038, 0.73, 1),\n",
       " (0.75385, 0.037, 0.74, 1),\n",
       " (0.75385, 0.035, 0.73, 1),\n",
       " (0.75385, 0.031, 0.71, 1),\n",
       " (0.7538, 0.12, 0.73, 1),\n",
       " (0.7538, 0.119, 0.73, 1),\n",
       " (0.7538, 0.118, 0.73, 2),\n",
       " (0.7538, 0.118, 0.73, 1),\n",
       " (0.7538, 0.117, 0.73, 2),\n",
       " (0.7538, 0.117, 0.73, 1),\n",
       " (0.7538, 0.116, 0.73, 1),\n",
       " (0.7538, 0.114, 0.73, 2),\n",
       " (0.7538, 0.113, 0.73, 1),\n",
       " (0.7538, 0.112, 0.73, 2),\n",
       " (0.7538, 0.11, 0.74, 2),\n",
       " (0.7538, 0.109, 0.73, 2),\n",
       " (0.7538, 0.104, 0.73, 2),\n",
       " (0.7538, 0.103, 0.73, 2),\n",
       " (0.7538, 0.103, 0.73, 1),\n",
       " (0.7538, 0.102, 0.73, 1),\n",
       " (0.7538, 0.097, 0.73, 1),\n",
       " (0.7538, 0.096, 0.73, 1),\n",
       " (0.7538, 0.095, 0.73, 1),\n",
       " (0.7538, 0.094, 0.73, 1),\n",
       " (0.7538, 0.093, 0.73, 1),\n",
       " (0.7538, 0.092, 0.73, 1),\n",
       " (0.7538, 0.091, 0.73, 1),\n",
       " (0.7538, 0.088, 0.73, 1),\n",
       " (0.7538, 0.087, 0.73, 1),\n",
       " (0.7538, 0.048, 0.71, 1),\n",
       " (0.7538, 0.037, 0.73, 1),\n",
       " (0.7538, 0.032, 0.71, 3),\n",
       " (0.7538, 0.03, 0.74, 1),\n",
       " (0.7538, 0.029, 0.74, 1),\n",
       " (0.7538, 0.024, 0.73, 3),\n",
       " (0.7538, 0.022, 0.73, 3),\n",
       " (0.75375, 0.148, 0.73, 1),\n",
       " (0.75375, 0.147, 0.73, 1),\n",
       " (0.75375, 0.146, 0.73, 1),\n",
       " (0.75375, 0.145, 0.73, 1),\n",
       " (0.75375, 0.144, 0.73, 1),\n",
       " (0.75375, 0.143, 0.73, 1),\n",
       " (0.75375, 0.142, 0.73, 1),\n",
       " (0.75375, 0.141, 0.73, 1),\n",
       " (0.75375, 0.14, 0.73, 1),\n",
       " (0.75375, 0.139, 0.73, 1),\n",
       " (0.75375, 0.138, 0.73, 1),\n",
       " (0.75375, 0.137, 0.73, 1),\n",
       " (0.75375, 0.136, 0.73, 1),\n",
       " (0.75375, 0.135, 0.73, 1),\n",
       " (0.75375, 0.134, 0.73, 1),\n",
       " (0.75375, 0.133, 0.73, 1),\n",
       " (0.75375, 0.123, 0.73, 1),\n",
       " (0.75375, 0.122, 0.73, 1),\n",
       " (0.75375, 0.121, 0.73, 1),\n",
       " (0.75375, 0.116, 0.73, 2),\n",
       " (0.75375, 0.108, 0.74, 2),\n",
       " (0.75375, 0.107, 0.74, 2),\n",
       " (0.75375, 0.099, 0.73, 1),\n",
       " (0.75375, 0.098, 0.73, 1),\n",
       " (0.75375, 0.094, 0.72, 2),\n",
       " (0.75375, 0.057, 0.71, 3),\n",
       " (0.75375, 0.056, 0.71, 3),\n",
       " (0.75375, 0.052, 0.71, 1),\n",
       " (0.75375, 0.046, 0.71, 3),\n",
       " (0.75375, 0.034, 0.74, 1),\n",
       " (0.75375, 0.034, 0.71, 3),\n",
       " (0.75375, 0.03, 0.73, 1),\n",
       " (0.75375, 0.029, 0.73, 1),\n",
       " (0.75375, 0.029, 0.71, 3),\n",
       " (0.75375, 0.021, 0.73, 3),\n",
       " (0.7537, 0.149, 0.73, 1),\n",
       " (0.7537, 0.132, 0.73, 1),\n",
       " (0.7537, 0.131, 0.73, 1),\n",
       " (0.7537, 0.13, 0.73, 1),\n",
       " (0.7537, 0.129, 0.73, 1),\n",
       " (0.7537, 0.128, 0.73, 1),\n",
       " (0.7537, 0.124, 0.73, 1),\n",
       " (0.7537, 0.113, 0.73, 2),\n",
       " (0.7537, 0.11, 0.73, 2),\n",
       " (0.7537, 0.105, 0.74, 2),\n",
       " (0.7537, 0.101, 0.73, 1),\n",
       " (0.7537, 0.1, 0.73, 1),\n",
       " (0.7537, 0.095, 0.72, 2),\n",
       " (0.7537, 0.059, 0.71, 3),\n",
       " (0.7537, 0.058, 0.71, 3),\n",
       " (0.7537, 0.055, 0.71, 3),\n",
       " (0.7537, 0.049, 0.71, 3),\n",
       " (0.7537, 0.045, 0.71, 3),\n",
       " (0.7537, 0.035, 0.71, 3),\n",
       " (0.7537, 0.034, 0.73, 1),\n",
       " (0.7537, 0.028, 0.71, 3),\n",
       " (0.7537, 0.024, 0.74, 3),\n",
       " (0.7537, 0.023, 0.73, 3),\n",
       " (0.7537, 0.019, 0.73, 3),\n",
       " (0.75365, 0.127, 0.73, 1),\n",
       " (0.75365, 0.126, 0.73, 1),\n",
       " (0.75365, 0.125, 0.73, 1),\n",
       " (0.75365, 0.108, 0.73, 2),\n",
       " (0.75365, 0.107, 0.73, 2),\n",
       " (0.75365, 0.106, 0.74, 2),\n",
       " (0.75365, 0.101, 0.74, 2),\n",
       " (0.75365, 0.093, 0.72, 2),\n",
       " (0.75365, 0.092, 0.72, 2),\n",
       " (0.75365, 0.06, 0.71, 3),\n",
       " (0.75365, 0.054, 0.71, 3),\n",
       " (0.75365, 0.053, 0.71, 1),\n",
       " (0.75365, 0.048, 0.71, 3),\n",
       " (0.75365, 0.047, 0.71, 3),\n",
       " (0.75365, 0.045, 0.71, 1),\n",
       " (0.75365, 0.033, 0.71, 1),\n",
       " (0.75365, 0.03, 0.71, 3),\n",
       " (0.75365, 0.028, 0.73, 1),\n",
       " (0.75365, 0.018, 0.73, 3),\n",
       " (0.75365, 0.014, 0.72, 3),\n",
       " (0.7536, 0.105, 0.73, 2),\n",
       " (0.7536, 0.102, 0.74, 2),\n",
       " (0.7536, 0.101, 0.73, 2),\n",
       " (0.7536, 0.065, 0.71, 1),\n",
       " (0.7536, 0.055, 0.71, 1),\n",
       " (0.7536, 0.054, 0.71, 1),\n",
       " (0.7536, 0.053, 0.71, 3),\n",
       " (0.7536, 0.052, 0.71, 3),\n",
       " (0.7536, 0.051, 0.71, 3),\n",
       " (0.7536, 0.05, 0.71, 3),\n",
       " (0.7536, 0.046, 0.71, 1),\n",
       " (0.7536, 0.044, 0.71, 1),\n",
       " (0.7536, 0.042, 0.71, 3),\n",
       " (0.7536, 0.036, 0.71, 3),\n",
       " (0.7536, 0.031, 0.71, 3),\n",
       " (0.7536, 0.028, 0.74, 1),\n",
       " (0.7536, 0.025, 0.72, 1),\n",
       " (0.7536, 0.023, 0.74, 3),\n",
       " (0.7536, 0.022, 0.74, 3),\n",
       " (0.75355, 0.106, 0.73, 2),\n",
       " (0.75355, 0.102, 0.73, 2),\n",
       " (0.75355, 0.083, 0.72, 2),\n",
       " (0.75355, 0.063, 0.71, 3),\n",
       " (0.75355, 0.057, 0.71, 1),\n",
       " (0.75355, 0.056, 0.71, 1),\n",
       " (0.75355, 0.05, 0.75, 1),\n",
       " (0.75355, 0.047, 0.71, 1),\n",
       " (0.75355, 0.044, 0.71, 3),\n",
       " (0.75355, 0.043, 0.71, 3),\n",
       " (0.75355, 0.039, 0.71, 3),\n",
       " (0.75355, 0.026, 0.71, 3),\n",
       " (0.75355, 0.021, 0.74, 3),\n",
       " (0.75355, 0.02, 0.73, 3),\n",
       " (0.75355, 0.019, 0.74, 3),\n",
       " (0.75355, 0.018, 0.74, 3),\n",
       " (0.75355, 0.017, 0.73, 3),\n",
       " (0.75355, 0.016, 0.73, 3),\n",
       " (0.7535, 0.1, 0.74, 2),\n",
       " (0.7535, 0.091, 0.72, 2),\n",
       " (0.7535, 0.088, 0.72, 2),\n",
       " (0.7535, 0.085, 0.72, 2),\n",
       " (0.7535, 0.084, 0.72, 2),\n",
       " (0.7535, 0.082, 0.72, 2),\n",
       " (0.7535, 0.073, 0.71, 3),\n",
       " (0.7535, 0.067, 0.71, 3),\n",
       " (0.7535, 0.065, 0.75, 1),\n",
       " (0.7535, 0.063, 0.71, 1),\n",
       " (0.7535, 0.061, 0.71, 3),\n",
       " (0.7535, 0.059, 0.75, 3),\n",
       " (0.7535, 0.059, 0.71, 1),\n",
       " (0.7535, 0.051, 0.75, 1),\n",
       " (0.7535, 0.041, 0.71, 3),\n",
       " (0.7535, 0.036, 0.71, 1),\n",
       " (0.7535, 0.017, 0.74, 3),\n",
       " (0.75345, 0.1, 0.73, 2),\n",
       " (0.75345, 0.099, 0.74, 2),\n",
       " (0.75345, 0.098, 0.74, 2),\n",
       " (0.75345, 0.098, 0.73, 2),\n",
       " (0.75345, 0.097, 0.74, 2),\n",
       " (0.75345, 0.097, 0.73, 2),\n",
       " ...]"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "090623d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet = []\n",
    "for l in open(\"pairs_Read.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    testSet.append([u,b, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "eb73e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "myPred = makePrediction2(0.057,generatePopSet(0.72),testSet)\n",
    "\n",
    "predictions = open('predictions_Read.csv', 'w')\n",
    "predictions.write('userID,bookID,prediction' + '\\n')\n",
    "for i in myPred:\n",
    "    predictions.write(str(i[0]) + ',' +  str(i[1]) +',' + str(i[2]) + '\\n')\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################################\n",
    "# Category prediction (CSE158 only)              #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e44b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them',\n",
    "'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
    "'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    "'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
    "'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf8b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "genreCountsPerUser = defaultdict(list)\n",
    "longestReview = 0\n",
    "mostReview0 = 0\n",
    "mostReview1 = 0\n",
    "mostReview2 = 0\n",
    "mostReview3 = 0\n",
    "mostReview4 = 0\n",
    "\n",
    "for d in readGz(\"train_Category.json.gz\"):\n",
    "    #we normalize the review length\n",
    "    if len(d['review_text']) > longestReview: \n",
    "        longestReview = len(d['review_text'])\n",
    "        \n",
    "    if len(genreCountsPerUser[d['user_id']]) == 0:\n",
    "        genreCountsPerUser[d['user_id']] = [0] * 5\n",
    "    genreCountsPerUser[d['user_id']][d['genreID']] += 1\n",
    "    data.append(d)\n",
    "    \n",
    "#for normalizing review counts\n",
    "for gc in genreCountsPerUser:\n",
    "    if genreCountsPerUser[gc][0] > mostReview0: \n",
    "        mostReview0 = genreCountsPerUser[gc][0]\n",
    "    if genreCountsPerUser[gc][1] > mostReview0: \n",
    "        mostReview1 = genreCountsPerUser[gc][1]\n",
    "    if genreCountsPerUser[gc][2] > mostReview0: \n",
    "        mostReview2 = genreCountsPerUser[gc][2]\n",
    "    if genreCountsPerUser[gc][3] > mostReview0: \n",
    "        mostReview3 = genreCountsPerUser[gc][3]\n",
    "    if genreCountsPerUser[gc][4] > mostReview0: \n",
    "        mostReview4 = genreCountsPerUser[gc][4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a696b146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'u75242413',\n",
       " 'review_id': 'r45843137',\n",
       " 'rating': 4,\n",
       " 'review_text': \"a clever book with a deeply troubling premise and an intriguing protagonist. Thompson's clean, sparse prose style kept each page feeling light even as some rather heavy existential questions dropped upon them. I enjoyed it. \\n and that cover design is boom-pow gorgeous.\",\n",
       " 'n_votes': 1,\n",
       " 'genre': 'mystery_thriller_crime',\n",
       " 'genreID': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20100ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the counts of words\n",
    "wordCount = defaultdict(int)\n",
    "bigramCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    unigrams = r.split()\n",
    "    bigrams = zip(unigrams, unigrams[1:-1])\n",
    "    for w in unigrams:\n",
    "        if w  not in stopwords:\n",
    "            wordCount[w] += 1\n",
    "    for w in bigrams:\n",
    "        #note there is no stopword check here, because now we have context\n",
    "        #for the stopwords\n",
    "        bigramCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts += [(bigramCount[w], str(w[0]) + ' ' + str(w[1])) for w in bigramCount]\n",
    "\n",
    "counts.sort()\n",
    "counts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save me some memory\n",
    "wordCount = 0\n",
    "bigramCount= 0\n",
    "unigrams = 0\n",
    "bigrams = 0\n",
    "counts = counts[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat(d, length):\n",
    "    #some characteristics that I want\n",
    "    otherfeats = []\n",
    "    # length of review\n",
    "    otherfeats.append(len(d['review_text'])/longestReview)\n",
    "    # a list of counts of number of genres a user has read\n",
    "    if len(genreCountsPerUser[d['user_id']]) == 0:\n",
    "        genreCountsPerUser[d['user_id']] = [0] * 5\n",
    "    otherfeats += genreCountsPerUser[d['user_id']]\n",
    "    otherfeats[1] = otherfeats[1]/mostReview0\n",
    "    otherfeats[2] = otherfeats[2]/mostReview1\n",
    "    otherfeats[3] = otherfeats[1]/mostReview2\n",
    "    otherfeats[4] = otherfeats[3]/mostReview3\n",
    "    otherfeats[5] = otherfeats[4]/mostReview4\n",
    "    \n",
    "    #bag of words\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation or not c in stopwords])\n",
    "    wordCount = defaultdict(int)\n",
    "    bigramCount = defaultdict(int)\n",
    "    unigrams = r.split()\n",
    "    bigrams = zip(unigrams, unigrams[1:-1])\n",
    "\n",
    "    punctionation = set(string.punctuation)\n",
    "    \n",
    "    for w in unigrams:\n",
    "        wordCount[w] += 1\n",
    "    for w in bigrams:\n",
    "        wordCount[str(w[0]) + ' ' + str(w[1])] += 1\n",
    "    bagofwords = [0] * length\n",
    "    for i in range(len(counts[:length])):\n",
    "        bagofwords[i] = wordCount[counts[i][1]]/counts[i][0]\n",
    "    \n",
    "    return [1] + otherfeats + bagofwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b168b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain = data[:9*len(data)//10]\n",
    "datavalid = data[9*len(data)//10:]\n",
    "ytrain = [ d['genreID'] for d in datatrain]\n",
    "yvalid = [ d['genreID'] for d in datavalid]\n",
    "\n",
    "traincorpus = [d['review_text'].lower() for d in datatrain]\n",
    "validcorpus = [d['review_text'].lower() for d in datavalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10e2169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datavalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8acb4370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accura(pred,yvalider):\n",
    "    correct = 0\n",
    "    for i in yvalider:\n",
    "        #print('pred: ' + str(i) + ' actual: '+ str(yvalider[i]))\n",
    "        if i == pred[i]:\n",
    "            correct += 1\n",
    "    return correct / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dcffe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularizer: 1000 dict size: 1000 accuracy: 0.3593\n",
      "regularizer: 1000 dict size: 2000 accuracy: 0.8351\n",
      "regularizer: 1000 dict size: 3000 accuracy: 0.2784\n",
      "regularizer: 1000 dict size: 4000 accuracy: 0.6377\n",
      "regularizer: 1000 dict size: 5000 accuracy: 0.7216\n",
      "regularizer: 1 dict size: 1000 accuracy: 0.3593\n",
      "regularizer: 1 dict size: 2000 accuracy: 0.8351\n",
      "regularizer: 1 dict size: 3000 accuracy: 0.2784\n",
      "regularizer: 1 dict size: 4000 accuracy: 0.6377\n",
      "regularizer: 1 dict size: 5000 accuracy: 0.6491\n",
      "regularizer: 0.001 dict size: 1000 accuracy: 0.3593\n",
      "regularizer: 0.001 dict size: 2000 accuracy: 0.3593\n",
      "regularizer: 0.001 dict size: 3000 accuracy: 0.3593\n",
      "regularizer: 0.001 dict size: 4000 accuracy: 0.3593\n",
      "regularizer: 0.001 dict size: 5000 accuracy: 0.3593\n"
     ]
    }
   ],
   "source": [
    "#dict size pipeline with tfidf\n",
    "mini = 1000\n",
    "maxi = 5001\n",
    "increment = 1000\n",
    "cs = [1000, 1, 1e-3]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "for c in cs:\n",
    "    count = mini\n",
    "    while count < maxi:\n",
    "        #getting model and training\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=stopwords, max_features=count)\n",
    "        bleX = vectorizer.fit_transform(traincorpus)\n",
    "        mod = linear_model.LogisticRegression(C=c, n_jobs=-1)\n",
    "        mod.fit(bleX, ytrain)\n",
    "    \n",
    "        #evaluating model\n",
    "        bleXvalid = vectorizer.fit_transform(validcorpus)\n",
    "        bleYpred = mod.predict(bleXvalid)\n",
    "\n",
    "        accc = accura(bleYpred, yvalid)\n",
    "    \n",
    "        accuracies.append((accc, count))\n",
    "        print('regularizer: '+ str(c) + ' dict size: ' + str(count) + ' accuracy: ' + str(accc))\n",
    "    \n",
    "        count+=increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "34fc80e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '100', '100 pages', ..., 'zone', 'zu', 'zum'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=stopwords, max_features=5000)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d402cef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ef966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ feat(d,2000) for d in data]\n",
    "Y = [ d['genreID'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b717508",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Xtrain \u001b[38;5;241m=\u001b[39m X[:\u001b[38;5;241m9\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m      2\u001b[0m ytrain \u001b[38;5;241m=\u001b[39m y[:\u001b[38;5;241m9\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(y)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m      3\u001b[0m Xvalid \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m9\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(X)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:291\u001b[0m, in \u001b[0;36mspmatrix.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix length is ambiguous; use getnnz()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or shape[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "Xtrain = X[:9*len(X)//10]\n",
    "ytrain = y[:9*len(y)//10]\n",
    "Xvalid = X[9*len(X)//10:]\n",
    "yvalid = y[9*len(y)//10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dec38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPred(c):\n",
    "    mod = linear_model.LogisticRegression(C=c)\n",
    "    mod.fit(Xtrain, Ytrain)\n",
    "    return mod.predict(Xvalid)\n",
    "def acc(pred):\n",
    "    correct = 0\n",
    "    for i in pred:\n",
    "        if i == Yvalid[i]:\n",
    "            correct += 1\n",
    "    return correct / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = getPred(1)\n",
    "acc7 = acc(pred)\n",
    "acc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dc611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f18b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the model\n",
    "mod = linear_model.LogisticRegression(C=1)\n",
    "mod.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0abb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting test data\n",
    "test = []\n",
    "for d in readGz(\"test_Category.json.gz\"):\n",
    "    test.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = [ feat(d,2000) for d in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4e466e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the prediction\n",
    "yPred = mod.predict(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a2bdab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 4, 3, 2, 2, 2, 2, 1, 1, 4, 1, 2, 1, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yPred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a74259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Category.csv\", 'w')\n",
    "predictions.write('userID,reviewID,prediction' + '\\n')\n",
    "\n",
    "for i in range(len(yPred)):\n",
    "    predictions.write(str(test[i]['user_id']) +',' + str(test[i]['review_id']) + ',' + str(yPred[i]) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca8c1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "815f8a88451bb9ed6445b8cdfafbc21867b1e3069831021276f9d3e049931616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
